{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d38625-18cc-4bc3-bc1a-31a89907c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.cuda.comm\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from models.ST_Former import GenerateModel\n",
    "from dataloader.dataset_NIA import train_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c79e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerateModel()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "model.to('cuda')\n",
    "train_on_gpu=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a49df2-435c-4c68-8f69-627e018232d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 8500\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA GeForce RTX 3090\n",
      "__CUDA Device Total Memory [GB]: 25.438322688\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "else:\n",
    "    print(\"Can't use CUDA\")\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8d38d",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e7939",
   "metadata": {},
   "source": [
    "FastAI training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbbfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torchvision\n",
    "\n",
    "import fastai\n",
    "from fastai.optimizer import OptimWrapper\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d20a472-2cd2-4f5e-a7e5-13af854f1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wandb\n",
    "from fastai.callback.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018f8961-8d73-463a-ac34-63f88affa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 16\n",
    "valid_size = 0.2\n",
    "data_set = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70174341-34db-4a76-ba7a-8829edf6c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "time_str = now.strftime(\"[%m-%d]-[%H:%M]-\")\n",
    "project_path = '/media/di/data/lee/nia/Former-DFER/nia/data/'\n",
    "log_txt_path = project_path + 'log/' + time_str + 'set' + str(data_set) + '-log.txt'\n",
    "log_curve_path = project_path + 'log/' + time_str + 'set' + str(data_set) + '-log.png'\n",
    "checkpoint_path = project_path + 'checkpoint/' + time_str + 'set' + str(data_set) + '-model.pth'\n",
    "best_checkpoint_path = project_path + 'checkpoint/' + time_str + 'set' + str(data_set) + '-model_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0fec4c-7dc0-43d4-a801-a25bbbdfecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a865db1-b12f-417b-8db2-377bbf655997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video number:2822\n",
      "video number:806\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data_loader(project_dir=project_path, \n",
    "                               data_set=data_set)\n",
    "test_data = test_data_loader(project_dir=project_path,\n",
    "                             data_set=data_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=num_workers,\n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=num_workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc36f13-a358-483d-abd4-d4004c633fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:22jnuoj0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fiery-disco-29</strong>: <a href=\"https://wandb.ai/fredericklee/nia/runs/22jnuoj0\" target=\"_blank\">https://wandb.ai/fredericklee/nia/runs/22jnuoj0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221108_155552-22jnuoj0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:22jnuoj0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff78d6798334621ab6173015ae7a2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666947233316023, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/di/data/500G_backup/nia/Former-DFER/wandb/run-20221108_155617-15mb9g08</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fredericklee/nia/runs/15mb9g08\" target=\"_blank\">magic-dream-30</a></strong> to <a href=\"https://wandb.ai/fredericklee/nia\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/fredericklee/nia/runs/15mb9g08?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd7b8349c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='nia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63173c43-e5bd-4930-911c-a0c0fd28ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_loader, test_loader)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learn = Learner(dls, model, loss_func=criterion, cbs=WandbCallback(), metrics=accuracy)#, opt_func=opt_func)#, cbs=[CudaCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d33c002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not gather input dimensions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.876085</td>\n",
       "      <td>1.999463</td>\n",
       "      <td>0.196030</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not gather input dimensions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.607912</td>\n",
       "      <td>1.656329</td>\n",
       "      <td>0.212159</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.598132</td>\n",
       "      <td>1.615565</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.584327</td>\n",
       "      <td>1.591616</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.577623</td>\n",
       "      <td>1.617465</td>\n",
       "      <td>0.224566</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.580426</td>\n",
       "      <td>1.616443</td>\n",
       "      <td>0.227047</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.575541</td>\n",
       "      <td>1.590098</td>\n",
       "      <td>0.246898</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.558449</td>\n",
       "      <td>1.601376</td>\n",
       "      <td>0.220844</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.564648</td>\n",
       "      <td>1.620848</td>\n",
       "      <td>0.213399</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.562739</td>\n",
       "      <td>1.594287</td>\n",
       "      <td>0.240695</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.565897</td>\n",
       "      <td>1.632263</td>\n",
       "      <td>0.251861</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.558946</td>\n",
       "      <td>1.621293</td>\n",
       "      <td>0.233251</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.547689</td>\n",
       "      <td>1.613750</td>\n",
       "      <td>0.236973</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.563901</td>\n",
       "      <td>1.605951</td>\n",
       "      <td>0.233251</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.565092</td>\n",
       "      <td>1.586832</td>\n",
       "      <td>0.229529</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.541024</td>\n",
       "      <td>1.596604</td>\n",
       "      <td>0.251861</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.559110</td>\n",
       "      <td>1.612343</td>\n",
       "      <td>0.218362</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.529919</td>\n",
       "      <td>1.571536</td>\n",
       "      <td>0.259305</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.546369</td>\n",
       "      <td>1.606322</td>\n",
       "      <td>0.249380</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.571018</td>\n",
       "      <td>1.562555</td>\n",
       "      <td>0.255583</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.525837</td>\n",
       "      <td>1.563091</td>\n",
       "      <td>0.272953</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.546523</td>\n",
       "      <td>1.592886</td>\n",
       "      <td>0.235732</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.526398</td>\n",
       "      <td>1.562550</td>\n",
       "      <td>0.282878</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.530219</td>\n",
       "      <td>1.607250</td>\n",
       "      <td>0.234491</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.519325</td>\n",
       "      <td>1.589519</td>\n",
       "      <td>0.266749</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.529030</td>\n",
       "      <td>1.593323</td>\n",
       "      <td>0.255583</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.526629</td>\n",
       "      <td>1.673292</td>\n",
       "      <td>0.234491</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.540512</td>\n",
       "      <td>1.595882</td>\n",
       "      <td>0.253102</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.544575</td>\n",
       "      <td>1.624381</td>\n",
       "      <td>0.235732</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.529216</td>\n",
       "      <td>1.615855</td>\n",
       "      <td>0.266749</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.535169</td>\n",
       "      <td>1.585161</td>\n",
       "      <td>0.267990</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.521680</td>\n",
       "      <td>1.566874</td>\n",
       "      <td>0.292804</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.533328</td>\n",
       "      <td>1.596156</td>\n",
       "      <td>0.227047</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.522741</td>\n",
       "      <td>1.583638</td>\n",
       "      <td>0.271712</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.540048</td>\n",
       "      <td>1.579447</td>\n",
       "      <td>0.260546</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.533265</td>\n",
       "      <td>1.565917</td>\n",
       "      <td>0.267990</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.551640</td>\n",
       "      <td>1.554127</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.509413</td>\n",
       "      <td>1.609157</td>\n",
       "      <td>0.250620</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.508295</td>\n",
       "      <td>1.551033</td>\n",
       "      <td>0.285360</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.558342</td>\n",
       "      <td>1.624541</td>\n",
       "      <td>0.248139</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.491154</td>\n",
       "      <td>1.541156</td>\n",
       "      <td>0.279156</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.524489</td>\n",
       "      <td>1.589455</td>\n",
       "      <td>0.270471</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.523525</td>\n",
       "      <td>1.561699</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.528104</td>\n",
       "      <td>1.575843</td>\n",
       "      <td>0.272953</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.529521</td>\n",
       "      <td>1.552994</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.525871</td>\n",
       "      <td>1.585565</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.549309</td>\n",
       "      <td>1.615384</td>\n",
       "      <td>0.245658</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.526119</td>\n",
       "      <td>1.574962</td>\n",
       "      <td>0.270471</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.509375</td>\n",
       "      <td>1.590900</td>\n",
       "      <td>0.259305</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.503229</td>\n",
       "      <td>1.575113</td>\n",
       "      <td>0.279156</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.487770</td>\n",
       "      <td>1.552414</td>\n",
       "      <td>0.280397</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.476129</td>\n",
       "      <td>1.529536</td>\n",
       "      <td>0.294045</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.473799</td>\n",
       "      <td>1.516322</td>\n",
       "      <td>0.299007</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.497209</td>\n",
       "      <td>1.498813</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.456361</td>\n",
       "      <td>1.473550</td>\n",
       "      <td>0.318859</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.456501</td>\n",
       "      <td>1.478415</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.449842</td>\n",
       "      <td>1.544435</td>\n",
       "      <td>0.279156</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.448625</td>\n",
       "      <td>1.448375</td>\n",
       "      <td>0.333747</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.441812</td>\n",
       "      <td>1.431190</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.421804</td>\n",
       "      <td>1.450238</td>\n",
       "      <td>0.323821</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.426248</td>\n",
       "      <td>1.430620</td>\n",
       "      <td>0.318859</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.409412</td>\n",
       "      <td>1.402315</td>\n",
       "      <td>0.332506</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.412969</td>\n",
       "      <td>1.413689</td>\n",
       "      <td>0.316377</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.440660</td>\n",
       "      <td>1.433650</td>\n",
       "      <td>0.344913</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.483166</td>\n",
       "      <td>1.489459</td>\n",
       "      <td>0.311414</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.447451</td>\n",
       "      <td>1.508511</td>\n",
       "      <td>0.302730</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.419779</td>\n",
       "      <td>1.433758</td>\n",
       "      <td>0.320099</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.390597</td>\n",
       "      <td>1.497398</td>\n",
       "      <td>0.308933</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.412202</td>\n",
       "      <td>1.417163</td>\n",
       "      <td>0.325062</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.395857</td>\n",
       "      <td>1.417244</td>\n",
       "      <td>0.318859</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.383813</td>\n",
       "      <td>1.416281</td>\n",
       "      <td>0.325062</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.380562</td>\n",
       "      <td>1.377473</td>\n",
       "      <td>0.343672</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.386427</td>\n",
       "      <td>1.388657</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.361725</td>\n",
       "      <td>1.376914</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.370847</td>\n",
       "      <td>1.413491</td>\n",
       "      <td>0.331266</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.545669</td>\n",
       "      <td>0.289082</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.431159</td>\n",
       "      <td>1.405025</td>\n",
       "      <td>0.316377</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.376601</td>\n",
       "      <td>1.370986</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.354321</td>\n",
       "      <td>1.406296</td>\n",
       "      <td>0.343672</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.382025</td>\n",
       "      <td>1.398763</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.377459</td>\n",
       "      <td>1.406004</td>\n",
       "      <td>0.336228</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.383574</td>\n",
       "      <td>1.358282</td>\n",
       "      <td>0.341191</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.352202</td>\n",
       "      <td>1.334010</td>\n",
       "      <td>0.356079</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.368770</td>\n",
       "      <td>1.350516</td>\n",
       "      <td>0.351117</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.340467</td>\n",
       "      <td>1.439321</td>\n",
       "      <td>0.326303</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.350733</td>\n",
       "      <td>1.458405</td>\n",
       "      <td>0.321340</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.321280</td>\n",
       "      <td>1.409636</td>\n",
       "      <td>0.343672</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.310313</td>\n",
       "      <td>1.367119</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.315929</td>\n",
       "      <td>1.385076</td>\n",
       "      <td>0.339950</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.381356</td>\n",
       "      <td>1.457432</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.355445</td>\n",
       "      <td>1.457428</td>\n",
       "      <td>0.330025</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.368662</td>\n",
       "      <td>1.398638</td>\n",
       "      <td>0.337469</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.368266</td>\n",
       "      <td>1.440217</td>\n",
       "      <td>0.328784</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.342862</td>\n",
       "      <td>1.478856</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.323679</td>\n",
       "      <td>1.493996</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.339872</td>\n",
       "      <td>1.385043</td>\n",
       "      <td>0.336228</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.309797</td>\n",
       "      <td>1.377089</td>\n",
       "      <td>0.352357</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.325163</td>\n",
       "      <td>1.317101</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.310089</td>\n",
       "      <td>1.353356</td>\n",
       "      <td>0.353598</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.365807</td>\n",
       "      <td>1.443259</td>\n",
       "      <td>0.323821</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.362730</td>\n",
       "      <td>1.393407</td>\n",
       "      <td>0.333747</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.339968</td>\n",
       "      <td>1.382549</td>\n",
       "      <td>0.351117</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.340634</td>\n",
       "      <td>1.353279</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.314110</td>\n",
       "      <td>1.324794</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.330832</td>\n",
       "      <td>1.333138</td>\n",
       "      <td>0.362283</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.337936</td>\n",
       "      <td>1.379886</td>\n",
       "      <td>0.341191</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.337984</td>\n",
       "      <td>1.339527</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.334566</td>\n",
       "      <td>1.342341</td>\n",
       "      <td>0.358561</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.352033</td>\n",
       "      <td>1.345696</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.332732</td>\n",
       "      <td>1.343125</td>\n",
       "      <td>0.357320</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.323583</td>\n",
       "      <td>1.333194</td>\n",
       "      <td>0.362283</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.311208</td>\n",
       "      <td>1.359155</td>\n",
       "      <td>0.343672</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.293813</td>\n",
       "      <td>1.362528</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.317898</td>\n",
       "      <td>1.381801</td>\n",
       "      <td>0.339950</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.316809</td>\n",
       "      <td>1.333622</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.294485</td>\n",
       "      <td>1.334518</td>\n",
       "      <td>0.353598</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.307364</td>\n",
       "      <td>1.491888</td>\n",
       "      <td>0.325062</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.299323</td>\n",
       "      <td>1.323617</td>\n",
       "      <td>0.358561</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.302020</td>\n",
       "      <td>1.326129</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.282533</td>\n",
       "      <td>1.353617</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.302025</td>\n",
       "      <td>1.307111</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.304131</td>\n",
       "      <td>1.336482</td>\n",
       "      <td>0.352357</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.313934</td>\n",
       "      <td>1.345956</td>\n",
       "      <td>0.362283</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.296370</td>\n",
       "      <td>1.308889</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.307320</td>\n",
       "      <td>1.354492</td>\n",
       "      <td>0.341191</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.331615</td>\n",
       "      <td>1.345828</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.304785</td>\n",
       "      <td>1.293297</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.299570</td>\n",
       "      <td>1.313903</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.298399</td>\n",
       "      <td>1.291601</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.319012</td>\n",
       "      <td>1.336596</td>\n",
       "      <td>0.349876</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.311705</td>\n",
       "      <td>1.307085</td>\n",
       "      <td>0.357320</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.267293</td>\n",
       "      <td>1.305948</td>\n",
       "      <td>0.352357</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.296638</td>\n",
       "      <td>1.333989</td>\n",
       "      <td>0.356079</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.304368</td>\n",
       "      <td>1.300738</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.294814</td>\n",
       "      <td>1.314305</td>\n",
       "      <td>0.353598</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.290933</td>\n",
       "      <td>1.296343</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.263806</td>\n",
       "      <td>1.283063</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.280423</td>\n",
       "      <td>1.378738</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.265188</td>\n",
       "      <td>1.278788</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.270545</td>\n",
       "      <td>1.330766</td>\n",
       "      <td>0.353598</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.296406</td>\n",
       "      <td>1.321347</td>\n",
       "      <td>0.353598</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.267162</td>\n",
       "      <td>1.292258</td>\n",
       "      <td>0.362283</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.286899</td>\n",
       "      <td>1.293380</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.266924</td>\n",
       "      <td>1.267775</td>\n",
       "      <td>0.361042</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.275379</td>\n",
       "      <td>1.284045</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.272061</td>\n",
       "      <td>1.281868</td>\n",
       "      <td>0.369727</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.273664</td>\n",
       "      <td>1.286660</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.280877</td>\n",
       "      <td>1.270378</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.284858</td>\n",
       "      <td>1.290389</td>\n",
       "      <td>0.364764</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.254017</td>\n",
       "      <td>1.275378</td>\n",
       "      <td>0.366005</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.284668</td>\n",
       "      <td>1.268149</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.268145</td>\n",
       "      <td>1.255820</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.256707</td>\n",
       "      <td>1.270445</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.279212</td>\n",
       "      <td>1.279631</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.265426</td>\n",
       "      <td>1.269873</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.253202</td>\n",
       "      <td>1.281937</td>\n",
       "      <td>0.367246</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.264956</td>\n",
       "      <td>1.264609</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.260192</td>\n",
       "      <td>1.281037</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.254962</td>\n",
       "      <td>1.259018</td>\n",
       "      <td>0.374690</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.252259</td>\n",
       "      <td>1.254472</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.251685</td>\n",
       "      <td>1.265097</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.254102</td>\n",
       "      <td>1.261948</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.266890</td>\n",
       "      <td>1.267263</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.266922</td>\n",
       "      <td>1.278780</td>\n",
       "      <td>0.367246</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.265706</td>\n",
       "      <td>1.280067</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.246568</td>\n",
       "      <td>1.264526</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.235501</td>\n",
       "      <td>1.271549</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.253482</td>\n",
       "      <td>1.275249</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.246397</td>\n",
       "      <td>1.268842</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.271122</td>\n",
       "      <td>1.274818</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.273220</td>\n",
       "      <td>1.274927</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.244458</td>\n",
       "      <td>1.270880</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.232979</td>\n",
       "      <td>1.281934</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.235420</td>\n",
       "      <td>1.265748</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.260769</td>\n",
       "      <td>1.268160</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.255365</td>\n",
       "      <td>1.267525</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.241681</td>\n",
       "      <td>1.261648</td>\n",
       "      <td>0.374690</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.260849</td>\n",
       "      <td>1.266747</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.241131</td>\n",
       "      <td>1.267728</td>\n",
       "      <td>0.373449</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.236694</td>\n",
       "      <td>1.265307</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.248187</td>\n",
       "      <td>1.264660</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.227839</td>\n",
       "      <td>1.264986</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.244989</td>\n",
       "      <td>1.270839</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.218849</td>\n",
       "      <td>1.264921</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.237723</td>\n",
       "      <td>1.269549</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.225145</td>\n",
       "      <td>1.274628</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.230307</td>\n",
       "      <td>1.270938</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.231142</td>\n",
       "      <td>1.274618</td>\n",
       "      <td>0.374690</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.236940</td>\n",
       "      <td>1.263900</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.248665</td>\n",
       "      <td>1.275499</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.250338</td>\n",
       "      <td>1.271821</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.265859</td>\n",
       "      <td>1.268233</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.233075</td>\n",
       "      <td>1.263561</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.240117</td>\n",
       "      <td>1.269999</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.239331</td>\n",
       "      <td>1.263863</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.244497</td>\n",
       "      <td>1.267486</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.237362</td>\n",
       "      <td>1.266709</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.242212</td>\n",
       "      <td>1.275313</td>\n",
       "      <td>0.374690</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.227500</td>\n",
       "      <td>1.269192</td>\n",
       "      <td>0.377171</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.221717</td>\n",
       "      <td>1.265283</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.239099</td>\n",
       "      <td>1.264763</td>\n",
       "      <td>0.378412</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39b863-a5c9-4b1e-913f-7e7ed131d81d",
   "metadata": {},
   "source": [
    "# Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e96eac4a-ba19-4cbb-820d-f80338557d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedWriter name='test.sav'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save(open(\"test.sav\", \"wb\"), with_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa6a7001-8deb-4a52-b05a-669a31b11f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.state_dict, \"test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "147db75a-c8ee-4081-bc75-44e99b487da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DataParallel(\n",
       "  (module): GenerateModel(\n",
       "    (s_former): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (spatial_transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (t_former): TFormer(\n",
       "      (spatial_transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4abb2603-4c43-4a5e-9571-fcc86e432742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Copies parameters and buffers from :attr:`state_dict` into\n",
       "this module and its descendants. If :attr:`strict` is ``True``, then\n",
       "the keys of :attr:`state_dict` must exactly match the keys returned\n",
       "by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
       "\n",
       "Args:\n",
       "    state_dict (dict): a dict containing parameters and\n",
       "        persistent buffers.\n",
       "    strict (bool, optional): whether to strictly enforce that the keys\n",
       "        in :attr:`state_dict` match the keys returned by this module's\n",
       "        :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
       "\n",
       "Returns:\n",
       "    ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
       "        * **missing_keys** is a list of str containing the missing keys\n",
       "        * **unexpected_keys** is a list of str containing the unexpected keys\n",
       "\n",
       "Note:\n",
       "    If a parameter or buffer is registered as ``None`` and its corresponding key\n",
       "    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
       "    ``RuntimeError``.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/nia/lib/python3.9/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.load_state_dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7576c4d4-f99d-4bbc-abc9-3c0a03cee46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0G\n",
      "-rw-rw-r--  1 di di  69M 11ì›”  9 13:35 test.pth\n",
      "drwxrwxr-x 14 di di 4.0K 11ì›”  9 13:35 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "-rw-rw-r--  1 di di  75K 11ì›”  9 13:33 train_nia_fastai.ipynb\n",
      "-rw-rw-r--  1 di di 207M 11ì›”  9 13:31 test.sav\n",
      "-rw-rw-r--  1 di di 876K 11ì›”  9 10:00 prepare_dataset_new.ipynb\n",
      "drwxrwxr-x 40 di di 4.0K 11ì›”  8 15:56 \u001b[01;34mwandb\u001b[0m/\n",
      "drwxrwxr-x  2 di di 4.0K 11ì›”  8 12:25 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
      "-rw-rw-r--  1 di di  11K 11ì›”  8 10:02 \u001b[01;31mmodel.tar.gz\u001b[0m\n",
      "drwxrwxr-x  7 di di 4.0K 11ì›”  8 10:01 \u001b[01;34m..\u001b[0m/\n",
      "-rw-rw-r--  1 di di  39K 11ì›”  8 09:09 CIFAR10_ResNet9_train.ipynb\n",
      "drwxrwxr-x  2 di di 4.0K 11ì›”  8 09:07 \u001b[01;34mdata\u001b[0m/\n",
      "-rw-rw-r--  1 di di 432K 11ì›”  7 20:39 train_nia_new.ipynb\n",
      "-rw-rw-r--  1 di di 5.3K 11ì›”  7 09:54 Make_datasets_new.ipynb\n",
      "drwxrwxr-x  4 di di 4.0K 11ì›”  7 09:45 \u001b[01;34mmodels\u001b[0m/\n",
      "drwxrwxr-x  4 di di 4.0K 11ì›”  4 15:52 \u001b[01;34mdataloader\u001b[0m/\n",
      "-rw-rw-r--  1 di di  26K 11ì›”  3 15:03 train_nia.ipynb\n",
      "drwxrwxr-x  2 di di 4.0K 11ì›”  3 15:00 \u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-rw-r--  1 di di 482M 11ì›”  2 11:55 cuda_11.0.2_450.51.05_linux.run\n",
      "-rw-rw-r--  1 di di  12K 11ì›”  1 16:12 Make_datasets.ipynb\n",
      "drwxrwxr-x  3 di di 4.0K 11ì›”  1 16:11 \u001b[01;34mmedia\u001b[0m/\n",
      "-rw-rw-r--  1 di di 7.9M 11ì›”  1 14:12 prepare_dataset.ipynb\n",
      "drwxrwxr-x  8 di di 4.0K 11ì›”  1 13:52 \u001b[01;34m.git\u001b[0m/\n",
      "-rw-rw-r--  1 di di 4.7K  9ì›” 15 21:38 utils.py\n",
      "drwxrwxr-x  7 di di 4.0K  9ì›” 15 21:34 \u001b[01;34mnia\u001b[0m/\n",
      "-rw-rw-r--  1 di di 134K  8ì›” 25 23:34 valid_nia.ipynb\n",
      "-rw-rw-r--  1 di di   16  8ì›” 25 23:31 .gitignore\n",
      "-rw-rw-r--  1 di di 4.8K  8ì›” 25 02:16 runner_helper.py\n",
      "drwxrwxr-x  2 di di 4.0K  8ì›” 24 23:22 \u001b[01;34mlog\u001b[0m/\n",
      "-rw-rw-r--  1 di di    8  8ì›” 24 20:30 clip_list00.txt\n",
      "drwxrwxr-x  3 di di 4.0K  8ì›” 24 17:27 \u001b[01;34mFAKE\u001b[0m/\n",
      "-rw-rw-r--  1 di di  13K  8ì›” 24 17:02 main_AFEW.py\n",
      "-rw-rw-r--  1 di di  13K  8ì›” 24 17:02 main_DFEW.py\n",
      "drwxrwxr-x  2 di di 4.0K  8ì›” 24 17:02 \u001b[01;34mannotation\u001b[0m/\n",
      "-rw-rw-r--  1 di di  305  8ì›” 24 17:02 DFEW_Five_Fold.sh\n",
      "-rw-rw-r--  1 di di 1.1K  8ì›” 24 17:02 LICENSE\n",
      "-rw-rw-r--  1 di di 1.5K  8ì›” 24 17:02 README.md\n",
      "-rw-rw-r--  1 di di 3.3G  5ì›”  5  2022 cuda_11.7.0_515.43.04_linux.run\n"
     ]
    }
   ],
   "source": [
    "ls -laht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cd274-b056-4948-9e5f-6b1fb4a4b22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "312595a83d408892f860676df71d6f6088a279ffd0ace127ffe1e58bd7b9393c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
